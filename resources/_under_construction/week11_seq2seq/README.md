[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yandexdataschool/practical_dl/blob/spring2019/week11_seq2seq/practice.ipynb)

### Materials
- Russian [lecture](https://www.youtube.com/watch?v=_XRBlhzb31U), [seminar](https://www.youtube.com/watch?v=s3oONja_SNQ)
- The only more-or-less reasonable lecture we found: [video](https://www.youtube.com/watch?v=QuELiw8tbx8). Contributions are welcome :)
- An awesome distill article explaining attention and augmented RNN - [article](https://distill.pub/2016/augmented-rnns/)
- A full [YSDA course](https://github.com/yandexdataschool/nlp_course) on natural language processing

### More materials
- Gumbel-softmax (usable for semi-hard attention) - [video](https://www.youtube.com/watch?v=wVkLM2KKHp8), [tutorial](http://blog.evjang.com/2016/11/tutorial-categorical-variational.html), [arxiv](https://arxiv.org/abs/1611.01144)
- Attention is all you need - on seq2seq architecture made entirely from attention - [video](https://www.youtube.com/watch?v=rBCqOTEfxvg), [arxiv](https://arxiv.org/abs/1706.03762)
- Convolutional + attentive seq2seq - [arxiv](https://arxiv.org/pdf/1705.03122.pdf)
- Recurrent dropout: [variational](https://arxiv.org/abs/1512.05287), [gate-specific](https://arxiv.org/abs/1603.05118)
- Normalization: [gate-specific](https://arxiv.org/abs/1603.09025), [layer-norm](https://arxiv.org/abs/1607.06450)
- Improved softmax operators: [post](http://sebastianruder.com/word-embeddings-softmax/index.html)
- Better tokenization for language-related stuff: [arxiv](https://arxiv.org/abs/1508.07909), [code](https://github.com/rsennrich/subword-nmt)

