{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced task: image captioning with visual attention\n",
    "\n",
    "![img](https://i.imgur.com/r3r0fS4.jpg)\n",
    "\n",
    "__This task__ walks you through all steps required to build an attentive image-to-captioning system. Except this time, there's no `<YOUR CODE HERE>`'s. You write all the code.\n",
    "\n",
    "You are free to approach this task in any way you want. Follow our step-by-step guide or abandon it altogether. Use the notebook or add extra .py files (remember to add them to your anytask submission). The only limitation is that your code should be readable and runnable top-to-bottom.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: image preprocessing (5 pts)\n",
    "\n",
    "First, you need to prepare images for captioning. Just like in the basic notebook, you are going to use a pre-trained image classifier from the model zoo. Let's go to the [`preprocess_data.ipynb`](./preprocess_data) notebook and change a few things there. This stage is mostly running the existing code with minor modiffications.\n",
    "\n",
    "1. Download the data someplace where you have enough space. You will need around 100Gb for the whole thing.\n",
    "2. Pre-compute and save Inception activations at the layer directly __before the average pooling__.\n",
    " - the correct shape should be `[batch_size, 2048, 8, 8]`. Your LSTM will attend to that 8x8 grid.\n",
    "\n",
    "\n",
    "__Note 1:__ Inception is great, but not the best model in the field. If you have enough courage, consider using ResNet or DenseNet from the same model zoo. Just remember that different models may require different image preprocessing.\n",
    "\n",
    "__Note 2:__ Running this model on CPU may take days. You can speed things up by processing data in parts using colab + google drive. Here's how you do that: https://colab.research.google.com/notebooks/io.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<...>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: sub-word tokenization (5 pts)\n",
    "\n",
    "While it is not strictly necessary for image captioning, you can generally improve generative text models by using sub-word units. There are several sub-word tokenizers available in the open-source (BPE, Wordpiece, etc).\n",
    "\n",
    "* __[recommended]__ BPE implementation you can use: [github_repo](https://github.com/rsennrich/subword-nmt). \n",
    "* Theory on how it works: https://arxiv.org/abs/1508.07909\n",
    "* We recommend starting with __4000 bpe rules__.\n",
    "* The result@@ ing lines will contain splits for rare and mis@@ typed words like this: ser@@ endi@@ pity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<...>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: define attentive decoder (5 pts)\n",
    "\n",
    "Your model works similarly to the normal image captioning decoder, except that it has an additional mechanism for peeping into image at each step. We recommend implementing this mechanism as a separate Attention layer, inheriting from `nn.Module`. Here's what it should do:\n",
    "\n",
    "![img](https://camo.githubusercontent.com/1f5d1b5def5ab2933b3746c9ef51f4622ce78b86/68747470733a2f2f692e696d6775722e636f6d2f36664b486c48622e706e67)\n",
    "\n",
    "\n",
    "__Input:__ 8x8=64 image encoder vectors $ h^e_0, h^e_1, h^e_2, ..., h^e_64$ and a single decoder LSTM hidden state $h^d$.\n",
    "\n",
    "* Compute logits with a 2-layer neural network with tanh activation (or anything similar)\n",
    "\n",
    "$$a_t = linear_{out}(tanh(linear_{e}(h^e_t) + linear_{d}(h_d)))$$\n",
    "\n",
    "* Get probabilities from logits, \n",
    "\n",
    "$$ p_t = {{e ^ {a_t}} \\over { \\sum_\\tau e^{a_\\tau} }} $$\n",
    "\n",
    "* Add up encoder states with probabilities to get __attention response__\n",
    "$$ attn = \\sum_t p_t \\cdot h^e_t $$\n",
    "\n",
    "You can now feed this $attn$ to the decoder LSTM in concatenation with previous token embeddings.\n",
    "\n",
    "__Note 1:__ If you need more information on how attention works, here's [a class in attentive seq2seq](https://github.com/yandexdataschool/nlp_course/tree/master/week04_seq2seq) from the NLP course.\n",
    "\n",
    "__Note 2:__ There's always a choice whether you initialize LSTM state with some image features or zeros. We recommend using zeros: it is a good way to debug whether your attention is working and it usually produces better-looking attention maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<...>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: training\n",
    "\n",
    "Up to 10 pts based on the model performance. \n",
    "The training procedure for your model is no different from the original non-attentive captioning from the base track: iterate minibatches, compute loss, backprop, use the optimizer.\n",
    "\n",
    "Feel free to use the [`basic track notebook`](./homework04_basic_part2_image_captioning) for \"inspiration\" :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<...>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final step: show us what it's capable of! (5 pts)\n",
    "\n",
    "The task is exactly the same as in the base track _(with the exception that you don't have to deal with salary prediction :) )_\n",
    "\n",
    "\n",
    "__Task: Find at least 10 images to test it on.__\n",
    "\n",
    "* Seriously, that's a part of the assignment. Go get at least 10 pictures for captioning\n",
    "* Make sure it works okay on __simple__ images before going to something more complex\n",
    "* Your pictures must feature both successful and failed captioning. Get creative :)\n",
    "* Use photos, not animation/3d/drawings, unless you want to re-train CNN network on anime\n",
    "* Mind the aspect ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply your network on images you've found\n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What else to try\n",
    "\n",
    "If you're made it this far you're awesome and you should know it already. All the tasks below are completely optional and may take a lot of your time. Proceed at your own risk\n",
    "\n",
    "#### Hard attention\n",
    "\n",
    "* There are more ways to implement attention than simple softmax averaging. Here's [a lecture](https://www.youtube.com/watch?v=_XRBlhzb31U) on that. \n",
    "* We recommend you to start with [gumbel-softmax](https://blog.evjang.com/2016/11/tutorial-categorical-variational.html) or [sparsemax](https://arxiv.org/abs/1602.02068) attention.\n",
    "\n",
    "#### Reinforcement learning\n",
    "\n",
    "* After your model has been pre-trained in a teacher forced way, you can tune for captioning-speific models like CIDEr.\n",
    "* Tutorial on RL for sequence models: [practical_rl week7](https://github.com/yandexdataschool/Practical_RL/tree/spring19/week7_seq2seq)\n",
    "* Theory: https://arxiv.org/abs/1612.00563\n",
    "\n",
    "#### Chilling out\n",
    "\n",
    "This is the final and the most advanced task in the DL course. And if you're doing this with the on-campus YSDA students, it should be late spring by now. There's got to be a better way to spend a few days than coding another deep learning model. If you have no idea what to do, ask Yandex. Or your significant other.\n",
    "\n",
    "![img](https://imgs.xkcd.com/comics/computers_vs_humans.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
