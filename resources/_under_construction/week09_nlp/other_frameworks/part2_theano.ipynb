{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salary prediction via Deep NLP methods (7 points)\n",
    "A recent Kaggle competition (merely 5 years) propose you to find out who is the most well-paid professional\n",
    "\n",
    "We are gonna solve regression task.\n",
    "\n",
    "Competition is available here: https://www.kaggle.com/c/job-salary-prediction\n",
    "\n",
    "![Hobby](https://imgs.xkcd.com/comics/extrapolating.png)\n",
    "\n",
    "In this notebook you will learn\n",
    " - Data preprocessing for NLP or the most annoying part of data scientist's job\n",
    " - Convolutional Neural Networks for texts\n",
    " - Constructing you NN with scripting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Dataset is consists of job data. Most of it is an unstructured text. You should predict annual salary\n",
    "\n",
    "Download here: https://yadi.sk/d/vVEOWPFY3NruT7 or from competition\n",
    "\n",
    "You need to download Train_rev1.zip and unpack it\n",
    "\n",
    "## Main fields\n",
    "\n",
    "Title - A freetext field supplied to us by the job advertiser as the Title of the job ad.  Normally this is a summary of the job title or role.\n",
    "\n",
    "FullDescription - The full text of the job ad as provided by the job advertiser.  Where you see ***s, we have stripped values from the description in order to ensure that no salary information appears within the descriptions.  There may be some collateral damage here where we have also removed other numerics.\n",
    "\n",
    "LocationRaw - The freetext location as provided by the job advertiser.\n",
    "\n",
    "LocationNormalized - Adzuna's normalised location from within our own location tree, interpreted by us based on the raw location.  Our normaliser is not perfect!\n",
    "\n",
    "ContractType - full_time or part_time, interpreted by Adzuna from description or a specific additional field we received from the advertiser.\n",
    "\n",
    "ContractTime - permanent or contract, interpreted by Adzuna from description or a specific additional field we received from the advertiser.\n",
    "\n",
    "Company - the name of the employer as supplied to us by the job advertiser.\n",
    "\n",
    "Category - which of 30 standard job categories this ad fits into, inferred in a very messy way based on the source the ad came from.  We know there is a lot of noise and error in this field.\n",
    "\n",
    "## Desiered field\n",
    "SalaryRaw - the freetext salary field we received in the job advert from the advertiser.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame.from_csv('Train_rev1.csv', index_col=None)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.ContractType = X.ContractType.astype(str)\n",
    "X.ContractTime = X.ContractTime.astype(str)\n",
    "X.Company = X.Company.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = X['SalaryNormalized'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del X['Id'], X['SalaryRaw'], X['SourceName'], X['SalaryNormalized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(Y, bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "At this very stage we will distill valuable data out of the dataset\n",
    "\n",
    "First of all - let's remove rare tokens and finalaize our dictionary\n",
    "\n",
    "We count all tokens ever occurred in any of `text_columns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_columns = list(X.columns)\n",
    "print text_columns\n",
    "categorial_colums = ['ContractType',\n",
    " 'ContractTime',\n",
    " 'Company',\n",
    " 'Category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code is below. \n",
    "\n",
    "Remember to apply .lower() to all strings before tokenization\n",
    "\n",
    "Consider using tqdm_notebook for not dying  during the iteration process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict\n",
    "tokenizer = RegexpTokenizer(r\"\\w+|\\d+\")\n",
    "\n",
    "#Dictionary of tokens\n",
    "token_counts = Counter()\n",
    "\n",
    "def tokenize(value):\n",
    "     \"\"\" Returns a list of string tokens\n",
    "    Arguments:\n",
    "    value -- string to tokenize\n",
    "    \"\"\"\n",
    "    return tokenizer.tokenize(value.lower())\n",
    "\n",
    "\n",
    "<FILL token_counts. YOUR CODE HERE>\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert token_counts.most_common(1)[0][1] in  range(2600000, 2700000)\n",
    "assert len(token_counts) in range(200000, 210000)\n",
    "print('Correct!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is the time to actually build token dict. We will use only words that occur more than `min_count` times in dataset\n",
    "\n",
    "Fill two dict mappings id->token and token->id\n",
    "\n",
    "**Minimum id is 2**, because 0 is reserved for padding and 1 is UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_count = 10\n",
    "\n",
    "tokens = <tokens from token_counts keys that had at least min_count occurrences throughout the dataset>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert type(tokens)==list\n",
    "assert len(tokens) in range(32000,34000)\n",
    "assert 'me' in tokens\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_size = len(tokens)+2\n",
    "id_to_word = dict()\n",
    "word_to_id = dict()\n",
    "\n",
    "\n",
    "token_to_id = <your code here>\n",
    "\n",
    "id_to_token = <your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert token_to_id['me'] != token_to_id['woods']\n",
    "assert token_to_id[id_to_token[42]]==42\n",
    "assert len(token_to_id)==len(tokens)\n",
    "assert 0 not in id_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(strings, token_to_id, UNK):\n",
    "    '''This function gets a string array and transforms it to padded token matrix\n",
    "    Remember to:\n",
    "     - Transform a string to list of tokens\n",
    "     - Transform each token to it ids (if not in the dict, replace with UNK)\n",
    "     - Pad each line to max_len'''\n",
    "    max_len = 0\n",
    "    token_matrix = []\n",
    "    \n",
    "    <YOUR CODE HERE>\n",
    "    return np.array(token_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = vectorize([\"Hello, adshkjasdhkas\", \"data\"], token_to_id, 1)\n",
    "assert test.shape==(2,2)\n",
    "assert (test[:,1]==(1,0)).all()\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you successfully completed all tasks by this moment, you get 2 pts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True deep learning\n",
    "\n",
    "Now we will define our convolutional neural network.\n",
    "\n",
    "We will think about categorical fields as a sequential - but we won't apply CNN to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES = \"\"\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import lasagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialaize some placeholders for data\n",
    "\n",
    "What size it should  be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "placeholders = dict()\n",
    "for col in text_columns:\n",
    "    placeholders[col] = T.matrix(col,dtype='int32')\n",
    "    \n",
    "true_y = T.vector(dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings are vector represetations for tokens. Basically it is just a table where each token (represented by it's id) has a vector representing its sense.\n",
    "\n",
    "It will be learned simultaneously with other layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_size = 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep nets\n",
    "Below we are gonna define some network architectures corresponding to each input (a column from a source data)\n",
    "\n",
    "`lasagne.layers.Conv1DLayer()` has three parameters. \n",
    " - incoming is a tensor with (batch_size, num_input_channels, input_length)\n",
    " - num_filters is an int number which means number of channels on the output\n",
    " - filter_size is an int number which means a size of input data which will be convoluted\n",
    "\n",
    "**Please, specify a padding='same'. This will prevent errors in case of low sequence lenght**\n",
    "`lasagne.layers.Conv1DLayer()` returns a tensor with dimension  [batch_size, time, filters]\n",
    "\n",
    "`lasagne.layers.MaxPool1DLayer()` is prerry much the same. The only thing you need to specify - pool size and strides.\n",
    "\n",
    "For global pooling you can use `class lasagne.layers.GlobalPoolLayer()`. **Don't forget to specify a pooling function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dream_neural_net(word_ids, name):\n",
    "    \"\"\"Function takes an int32 matrix [batch_size, time] and returnes infered [batch_size, dense_vector] \n",
    "    matrix with a conv and pooling layers, finished by a global pooling\"\"\"\n",
    "    output = lasagne.layers.InputLayer([None, None], word_ids)\n",
    "    output = lasagne.layers.EmbeddingLayer(output, input_size=dict_size, output_size=embeddings_size)\n",
    "    <YOUR CODE HERE>\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dream_neural_net_categorial(word_ids, name):\n",
    "    \"\"\"Function takes an int32 matrix [batch_size, time] and returnes infered [batch_size, latent_dim] \n",
    "    matrix without a conv and pooling layers, only dense layers\"\"\"\n",
    "    <YOUR CODE HERE>\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to combine all architectures. In a dict below you can match input type and an architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nets_types = {'FullDescription': dream_neural_net, \n",
    " 'LocationRaw': dream_neural_net, \n",
    " 'Title': dream_neural_net, \n",
    " 'LocationNormalized': dream_neural_net, \n",
    " 'ContractType': dream_neural_net_categorial, \n",
    " 'ContractTime': dream_neural_net_categorial, \n",
    " 'Company': dream_neural_net_categorial, \n",
    " 'Category':dream_neural_net_categorial\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#here we apply specified architecture for each input\n",
    "outputs_to_concat = [nets_types[name](word_ids, name) for name, word_ids in placeholders.items()]\n",
    "dense_repr = lasagne.layers.ConcatLayer(outputs_to_concat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert len(dense_repr.output_shape)==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce_to_number(dense_inputs):\n",
    "    \"\"\"\n",
    "    Function reduces a concated matrix [batch_size, latent_dim] and returns a net output \n",
    "    which will be directly used for inference\n",
    "    \"\"\"\n",
    "    <YOUR CODE HERE>\n",
    "    output = lasagne.layers.DenseLayer(output,<YOUR CODE HERE>)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net_output = reduce_to_number(dense_repr)\n",
    "predicted_y = lasagne.layers.get_output(net_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_y = predicted_y.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "Remember - we have a regression task, what would be the loss?\n",
    "\n",
    "Also we will estimate a target metric for a competition - **Mean absolute error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = T.mean((predicted_y - true_y)**2)\n",
    "mean_abs_error = T.mean(T.abs_(predicted_y - true_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = lasagne.updates.adam(loss, lasagne.layers.get_all_params(net_output)) # <your code here>\n",
    "\n",
    "train_op = theano.function(list(placeholders.values())+[true_y], [loss, mean_abs_error], updates=optimizer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validate_op = theano.function(list(placeholders.values())+[true_y], [loss, mean_abs_error] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process\n",
    "\n",
    "The last thing before we can run the whole monster - define train process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_batches(X, Y=None):\n",
    "\"\"\"Takes a part of pandas DF\n",
    "    Returns a pair (X_batch, Y_batch) or only X_batch, where \n",
    "     - X_batch is a dict {key: value} - where a key is name of column and a value is \n",
    "    a matrix which will be passed to corresponding input.\n",
    "     - Y_batch is just a vector with output values\n",
    "    \"\"\"\n",
    "    size = len(X)\n",
    "    i = 0\n",
    "    while i < len(X):\n",
    "        <YOUR CODE HERE>\n",
    "        \n",
    "        if Y is None:\n",
    "            yield X_batch \n",
    "        else:\n",
    "            yield X_batch, Y_batch\n",
    "        i+=batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inputs(X_batch,Y_batch=None):\n",
    "    \"\"\"Function takes a batch from iterate_batches and returns a train arguments with placeholders\"\"\"\n",
    "    feed_dct = [X_batch[k].astype('int32') for k in text_columns]\n",
    "    if Y_batch is not None:\n",
    "        feed_dct.append(Y_batch.astype('int32'))\n",
    "    return feed_dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate():\n",
    "    \"\"\"Here you go over X_val and Y_val and count mean errors.\n",
    "    Use iterate_batches and get_input\"\"\"\n",
    "    MSE, AE = 0, 0\n",
    "    batches = 0\n",
    "    <YOUR CODE HERE>\n",
    "    MSE/=batches\n",
    "    AE/=batches\n",
    "    return (MSE, AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    \"Function inferes prediction by dict of inputs\\\"\n",
    "    return sess.run(predicted_y, get_dict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for first_batch in iterate_batches(X_train, Y_train):\n",
    "    break\n",
    "assert len(first_batch) == 2\n",
    "assert type(first_batch[0]) == dict\n",
    "assert first_batch[1].shape[0]==batch_size\n",
    "assert np.unique([inp.shape[0] for inp in first_batch[0].values()])==batch_size\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MSE, AE = validate()\n",
    "assert MSE < 1e10\n",
    "assert AE < 50000\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some hyper-params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_batches = len(X_train)/batch_size\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Finally it is time to run training\n",
    "\n",
    "First, shuffle the data\n",
    "\n",
    "\n",
    "By the way, if the trainig processes starts and you achive at leasrt **20k** AE error on validation, you get additional **(3 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MSE, AE = validate()\n",
    "assert AE < 20000\n",
    "print (\"I earned 3 pts with %s absolute error!\" % AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = np.random.permutation(len(X_train))\n",
    "X_train = X_train.iloc[p]\n",
    "Y_train = Y_train.iloc[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for j in range(num_epochs):\n",
    "    for i, (X_batch, Y_batch) in  enumerate(iterate_batches(X_train, Y_train)):\n",
    "        current_loss, abs_error =  train_op(*get_inputs(X_batch,Y_batch))\n",
    "        print \"Current step: %s. Current loss is %s. Absolute error is %s\" % (j, current_loss, abs_error)\n",
    "        if i%30==0:\n",
    "            print(\"Validation. MSE: %s, AE: %s\" % validate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Further experiments and home assignment\n",
    " - **(1 pt)** Add visualisation of train and val loss. Try to use some smoothing to make plots more readable \n",
    " - **(2 pt)** Try different CNN architectures. Vary kernel size, number of filters. Find out if there is some change to a training process.\n",
    " - Try to use different architectures for different inputs. Maybe a smaller architecture would be fine for description field and more complex for a title.\n",
    " - Find out the best **embedding size** value \n",
    " - Add dropout, and some regularisation \n",
    " \n",
    " **(2++ pts) for all experimenting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See in the next series\n",
    " - Recurrent neural networks\n",
    "  - Why everybody like them\n",
    "  - Why everybody hate them\n",
    " - Attention for text processing\n",
    "  - How to boost your NLP model performance by implementing recent DL paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
