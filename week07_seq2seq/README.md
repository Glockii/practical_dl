__[slides](https://yadi.sk/i/Fey_u0IrbdICpg)__


### Materials
- Russian [lecture](https://www.youtube.com/watch?v=_XRBlhzb31U), [seminar](https://www.youtube.com/watch?v=s3oONja_SNQ)
- The only more-or-less reasonable lecture we found: [video](https://www.youtube.com/watch?v=QuELiw8tbx8). Contributions are welcome :)
- An awesome distill article explaining attention and augmented RNN - [article](https://distill.pub/2016/augmented-rnns/)

### More materials
- Gumbel-softmax (usable for semi-hard attention) - [video](https://www.youtube.com/watch?v=wVkLM2KKHp8), [tutorial](http://blog.evjang.com/2016/11/tutorial-categorical-variational.html), [arxiv](https://arxiv.org/abs/1611.01144)
- Attention is all you need - on seq2seq architecture made entirely from attention - [video](https://www.youtube.com/watch?v=rBCqOTEfxvg), [arxiv](https://arxiv.org/abs/1706.03762)
- Convolutional + attentive seq2seq - [arxiv](https://arxiv.org/pdf/1705.03122.pdf)
- Recurrent dropout: [variational](https://arxiv.org/abs/1512.05287), [gate-specific](https://arxiv.org/abs/1603.05118)
- Normalization: [gate-specific](https://arxiv.org/abs/1603.09025), [layer-norm](https://arxiv.org/abs/1607.06450)
- Improved softmax operators: [post](http://sebastianruder.com/word-embeddings-softmax/index.html)
- Better tokenization for language-related stuff: [arxiv](https://arxiv.org/abs/1508.07909), [code](https://github.com/rsennrich/subword-nmt)

### Homework assignment

There's a main assignment on image captioning (both torch, tf and theano) that's worth 20 points.


Helper urls:
- MS COCO challenge: http://mscoco.org/dataset/#overview
- Im2latex: https://openai.com/requests-for-research/#im2latex

